{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPk4AWMgUu5q29hzfbtL/e4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achuntya/ML-project/blob/main/week_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RZCP0FcWvBqm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ReLU(Z):\n",
        "    return np.max(0,Z)\n",
        "\n",
        "\n",
        "\n",
        "def softmax(Z):\n",
        "    A = np.exp(Z)/sum(np.exp(Z))\n",
        "    return A\n"
      ],
      "metadata": {
        "id": "QvYdBVUwvQLf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        # initialized basic stats of NN\n",
        "        self.input_size=input_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.output_size=output_size\n",
        "        self.learning_rate=learning_rate\n",
        "\n",
        "        #initialized weights and biases\n",
        "        self.W1=np.random.randn(hidden_size, input_size)*0.01\n",
        "        self.b1= np.zeros((hidden_size, 1))\n",
        "        self.W2=np.random.randn(output_size, hidden_size)*0.01\n",
        "        self.b2=np.zeros((output_size, 1))\n",
        "\n",
        "        #initialized activations and gradients\n",
        "        self.AO=None\n",
        "        self.Z1=None\n",
        "        self.A1=None\n",
        "        self.Z2=None\n",
        "        self.A2=None\n",
        "        self.dW2=None\n",
        "        self.db2=None\n",
        "        self.dW1=None\n",
        "        self.db1=None\n",
        "\n",
        "    # do the forward pass and evaluate the values of A0, Z1, A1, Z2, A2\n",
        "    def forward_propagation(self, X):\n",
        "      for i in range(self.input_size):\n",
        "        for j in range(self.hidden_size):\n",
        "          self.Z1[i][j] = np.dot(self.W1[j][i],X[i]) + self.b1[j]\n",
        "          self.A1[i][j] = ReLU(self.Z1[i][j])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # convert the input y, into a one hot encoded array.\n",
        "    '''\n",
        "    one hot encoding is:\n",
        "    you have an array with values [2, 5, 6] and you know the max value can be 8, then one hot encoded array will be:\n",
        "    [[0,0,1,0,0,0,0,0,0], [0,0,0,0,0,1,0,0,0], [0,0,0,0,0,0,1,0,0]]\n",
        "    Note that the index 2, 5, 6 have values 1 and all others have values 0\n",
        "    '''\n",
        "    def one_hot(self, y):\n",
        "        self.y_one_hot = np.zeros((self.output_size,y.shape[0]))\n",
        "        for i in range(y.shape[0]):\n",
        "          self.y_one_hot[y[i]][i] = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # calculate the derivative of the loss function with respect to W2, b2, W1, b1 in dW2, db2, dW1, db1 respectively\n",
        "    def backward_propagation(self, X, y):\n",
        "        self.one_hot(y)\n",
        "        self.dZ2 = self.A2-self.y_one_hot\n",
        "        self.dW2 = np.dot(self.dZ2,self.A1.T)/self.m\n",
        "        self.db2 = np.sum(self.dZ2,axis=1,keepdims = True)/self.m\n",
        "        self.dZ1 = np.dot(self.W2.T,self.dZ2)*ReLU_derivative(self.z1)\n",
        "        self.dW1 = np.dot(self.dZ1,X.T)/self.m\n",
        "        self.db1 = np.sum(self.dZ1,axis=1,keepdims = True)/self.m\n",
        "        return self.dW2,self.db2,self.dW1,self.db1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # update the parameters W1, W2, b1, b2\n",
        "    def update_params(self):\n",
        "      W1 = self.W1 - self.learning_rate*self.dW1\n",
        "      W2 = self.W2-self.learning_rate*self.dW2\n",
        "      b1 = self.b1-self.learning_rate*self.db1\n",
        "      b2 = self.b2-self.learning_rate*self.db2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # get the predictions for the dataset\n",
        "    def get_predictions(self):\n",
        "      self.A2 = softmax(self.Z2)\n",
        "      return np.argmax(self.A2,0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # get the accuracy of the model on the dataset\n",
        "    def get_accuracy(self, X, y):\n",
        "       predictions = self.get_predictions()\n",
        "       return np.sum(predictions == y)/y.shape[0]\n",
        "\n",
        "\n",
        "    # run gradient descent on the model to get the values of the parameters\n",
        "    def gradient_descent(self, X, y, iters=1000):\n",
        "        self.m = X.shape[0]\n",
        "        self.Z1 = np.zeros((self.hidden_size,self.m))\n",
        "        self.A1 = np.zeros((self.hidden_size,self.m))\n",
        "        self.Z2 = np.zeros((self.output_size,self.m))\n",
        "        self.A2 = np.zeros((self.output_size,self.m))\n",
        "        for i in range(iters):\n",
        "            self.forward_propagation(X)\n",
        "            self.backward_propagation(y)\n",
        "            self.update_params()\n",
        "\n",
        "            if i%100 == 0:\n",
        "                print(f\"Iteration:{i},Accuracy:{self.get_accuracy(X,y)}\")\n",
        "                self.show_predictions(X,y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # evaluate loss using cross-entropy-loss formula.\n",
        "    def cross_entropy_loss(self, X, y):\n",
        "        self.one_hot(y)\n",
        "        self.loss = -np.sum(self.y-one_hot*np.log(self.A2))/self.m\n",
        "        return self.loss\n",
        "\n",
        "    # Let me help a bit hehe :)\n",
        "    def show_predictions(self, X, y, num_samples=10):\n",
        "        random_indices = np.random.randint(0, X.shape[0], size=num_samples)\n",
        "\n",
        "        for index in random_indices:\n",
        "            sample_image = X[index, :].reshape((28, 28))\n",
        "            plt.imshow(sample_image, cmap='gray')\n",
        "            plt.title(f\"Actual: {y[index]}, Predicted: {self.get_predictions()[index]}\")\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "dX0KvSh51Udk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "REQXddIGK8D9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}